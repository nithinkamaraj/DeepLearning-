{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac0f13f-cec0-4f89-8a31-b8d5edfefebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch   # Create tensor \n",
    "import torch.nn as nn # to weights and biases as part of neural network \n",
    "import torch.nn.functional as F # activation functions \n",
    "from torch.optim import Adam    # Adam is to fit the data, similar to stochastic gradient descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e3d2a71-b9c5-4383-8240-9ad422f90128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader # Makes things easier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "64365b13-8d93-418c-8704-f3879a37146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMbyHand(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        \n",
    "         super().__init__() # Intialization using lighting module \n",
    "         mean = torch.tensor(0.0) # mean of standard normal distribution ( this is taken to be zero)\n",
    "         std =  torch.tensor(1.0) # std of normal distribuiton is set at one \n",
    "\n",
    "         self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True) # normal give as value betwen 0 to 1 from normal distribution graph\n",
    "         self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)  \n",
    "         self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)   \n",
    "    \n",
    "         self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True) \n",
    "         self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)  \n",
    "         self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True) \n",
    "        \n",
    "         self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True) \n",
    "         self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)  \n",
    "         self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True) \n",
    "\n",
    "         self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True) \n",
    "         self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)  \n",
    "         self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True) \n",
    "\n",
    "        \n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):    \n",
    "        \n",
    "        #Does the LSTM Caclulatuion \n",
    "    \n",
    "        # Forget Gate : This calculates percentage of the long term to remember \n",
    "        long_remember_percent      =  torch.sigmoid((short_memory*self.wlr1) + (input_value*self.wlr2) + self.blr1)\n",
    "        \n",
    "        # Input Gate :  updating the cell state\n",
    "        potential_remember_percent =  torch.sigmoid(short_memory*self.wpr1 + input_value*self.wpr2 + self.bpr1)\n",
    "        potential_memory           =  torch.tanh(short_memory*self.wp1 + input_value*self.wp2 + self.bp1)\n",
    "        updated_long_memory        =  long_remember_percent*long_memory + potential_remember_percent*potential_memory \n",
    "\n",
    "        # Output Gate: calcualating new short term memory \n",
    "        output_percent            = torch.sigmoid(short_memory*self.wo1 + input_value*self.wo2 + self.bo1)\n",
    "        updated_short_memory       = torch.tanh(updated_long_memory)*output_percent\n",
    "\n",
    "        return ([updated_long_memory, updated_short_memory])\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Make as forward pass in the unrolled lstm \n",
    "        \n",
    "        long_memory =0\n",
    "        short_memory =0\n",
    "        \n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "\n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "\n",
    "        return short_memory\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # Configure Adam optimizer \n",
    "\n",
    "        return Adam(self.parameters())\n",
    "        \n",
    "    def training_step( self, batch, batch_idx):\n",
    "        \n",
    "        #  calculate loss and log the porgress\n",
    "\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i[0])\n",
    "        loss = (output_i - label_i)**2\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        if(label_i==0):  \n",
    "            self.log(\"out_0\", output_i) # company A\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i) # company B\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c84954c2-d789-45ed-8cdc-7f01a3364c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare observed and peridicated values\n",
      "company A observed =0, predicted = tensor(-0.2322)\n"
     ]
    }
   ],
   "source": [
    "model = LSTMbyHand()\n",
    "print(\"Compare observed and peridicated values\")\n",
    "print(\"company A observed =0, predicted =\", model(torch.tensor([0.,0.5,0.25,1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fd27f4af-c45c-4bdd-88c4-9ced45d844e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare observed and predicted values\n",
      "Company A observed = 0, predicted = tensor(0.0645)\n"
     ]
    }
   ],
   "source": [
    "model = LSTMbyHand()\n",
    "\n",
    "print(\"Compare observed and predicted values\")\n",
    "\n",
    "input_data = torch.tensor([0., 0.5, 0.25, 1.])  # Ensure correct tensor type\n",
    "predicted_value = model(input_data).detach()\n",
    "\n",
    "print(\"Company A observed = 0, predicted =\", predicted_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "638398a8-b4ec-4abe-89be-245125ca87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "46c49fc6-03dc-4473-8fe2-79ba95b71d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "No `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m) \u001b[38;5;66;03m# with default learning rate, 0.001 (this tiny learning rate makes learning slow)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloaders\u001b[38;5;241m=\u001b[39mdataloader)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    541\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    571\u001b[0m     ckpt_path,\n\u001b[0;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:932\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector\u001b[38;5;241m.\u001b[39m_attach_model_callbacks()\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector\u001b[38;5;241m.\u001b[39m_attach_model_logging_functions()\n\u001b[1;32m--> 932\u001b[0m _verify_loop_configurations(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    934\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;66;03m# SET UP THE TRAINER\u001b[39;00m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    937\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: setting up strategy environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:36\u001b[0m, in \u001b[0;36m_verify_loop_configurations\u001b[1;34m(trainer)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected: Trainer state fn must be set before validating loop configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[1;32m---> 36\u001b[0m     __verify_train_val_loop_configuration(trainer, model)\n\u001b[0;32m     37\u001b[0m     __verify_manual_optimization_support(trainer, model)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mVALIDATING:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:59\u001b[0m, in \u001b[0;36m__verify_train_val_loop_configuration\u001b[1;34m(trainer, model)\u001b[0m\n\u001b[0;32m     57\u001b[0m has_optimizers \u001b[38;5;241m=\u001b[39m is_overridden(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigure_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_optimizers:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# verify minimum validation requirements\u001b[39;00m\n\u001b[0;32m     65\u001b[0m has_val_loader \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mval_loop\u001b[38;5;241m.\u001b[39m_data_source\u001b[38;5;241m.\u001b[39mis_defined()\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: No `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined."
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=2000) # with default learning rate, 0.001 (this tiny learning rate makes learning slow)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ff835-b8dc-454d-9891-8fa109566c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
